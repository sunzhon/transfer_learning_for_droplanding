{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c53690aa-8e52-4c9d-91e1-94c908a49698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./../\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import h5py\n",
    "import vicon_imu_data_process.process_rawdata as pro_rd\n",
    "import estimation_assessment.scores as es_as\n",
    "\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import re\n",
    "import json\n",
    "\n",
    "from vicon_imu_data_process.const import FEATURES_FIELDS, LABELS_FIELDS, DATA_PATH\n",
    "from vicon_imu_data_process.const import DROPLANDING_PERIOD, RESULTS_PATH\n",
    "from vicon_imu_data_process import const\n",
    "from vicon_imu_data_process.dataset import *\n",
    "\n",
    "from estimation_models.rnn_models import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "import time as localtimepkg\n",
    "\n",
    "from estimation_study import *\n",
    "from estimation_assessment.visualization import *\n",
    "#import pdb\n",
    "import re\n",
    "\n",
    "from fireTS.models import NARX\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9848ab5-ce2c-4c0a-ae46-19bdd3760310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P_15_liuzhaoyu', 'P_16_zhangjinduo', 'P_17_congyuanqi', 'P_18_hezhonghai', 'P_19_xiongyihui', 'P_20_xuanweicheng', 'P_21_wujianing', 'P_22_zhangning', 'P_23_wangjinhong', 'P_08_zhangboyuan', 'P_14_hunan', 'P_24_liziqing', 'P_10_dongxuan', 'P_11_liuchunyu', 'P_13_xulibang']\n",
      "DO NOT synchronize features and labels: False\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "\n",
    "hyperparams={}\n",
    "hyperparams['raw_dataset_path']= os.path.join(DATA_PATH,'features_labels_rawdatasets.hdf5')\n",
    "#i) define hyperparams values: subject, columns_names\n",
    "hyperparams['landing_manner']='double_legs'\n",
    "hyperparams['target_leg']='R'\n",
    "hyperparams['subjects_trials'] = pro_rd.set_subjects_trials(selected=True, landing_manner='double_legs')\n",
    "#ii) set data fields in 'columns_names'\n",
    "labels_fields = ['R_GRF_Z']\n",
    "#labels_fields = ['R_FOOT_Accel_Z','R_GRF_X','R_GRF_Y','R_GRF_Z']\n",
    "hyperparams['features_names'] = FEATURES_FIELDS\n",
    "hyperparams['labels_names'] = labels_fields\n",
    "hyperparams['columns_names'] = hyperparams['features_names'] + hyperparams['labels_names'] + ['TIME']\n",
    "hyperparams['syn_features_labels'] = False\n",
    "\n",
    "# set subjects and trials\n",
    "#hyperparams['subjects_trials'] = set_subjects_trials(landing_manner=landing_manner, target_leg=target_leg)\n",
    "\n",
    "subjects_trials_data, scaled_subjects_trials_data, scaler = load_normalize_data(hyperparams=hyperparams, scaler='standard')\n",
    "\n",
    "dataset = scaled_subjects_trials_data['P_15_liuzhaoyu']['01']\n",
    "dataset.shape\n",
    "x=dataset[:,:-1]\n",
    "y=dataset[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "189d3c6e-e0b9-478d-abfe-f4b839dfc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha:L2的参数：MLP是可以支持正则化的，默认为L2，具体参数需要调整\n",
    "# hidden_layer_sizes=(5, 2) hidden层2层,第一层5个神经元，第二层2个神经元)，2层隐藏层，也就有3层神经网络\n",
    "#clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "#'identity'，无操作激活，对实现线性瓶颈很有用，返回f（x）= x\n",
    "#'logistic'，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））。\n",
    "#'tanh'，双曲tan函数，返回f（x）= tanh（x）。\n",
    "#'relu'，整流后的线性单位函数，返回f（x）= max（0，x）\n",
    "model_mlp = MLPRegressor(\n",
    "        hidden_layer_sizes = (100,100),  activation=(('relu','identity')), solver='adam', alpha=0.0001, batch_size='auto',\n",
    "        learning_rate ='constant', learning_rate_init=0.001, power_t=0.5, max_iter=5000, shuffle=False,\n",
    "        random_state=1, tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "        early_stopping=True, beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "    )\n",
    "\n",
    "mdl = NARX(model_mlp, auto_order=2, exog_order=73*[2], exog_delay=73*[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7fa8b010-e409-42f7-ac91-85659db46a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The activation '('relu', 'identity')' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-48e055340f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Traning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/fireTS/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_and_preprocess_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \"\"\"\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Validate input parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             raise ValueError(\"hidden_layer_sizes must be > 0, got %s.\" %\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_hyperparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# raise ValueError if not registered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mACTIVATIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             raise ValueError(\"The activation '%s' is not supported. Supported \"\n\u001b[0m\u001b[1;32m    420\u001b[0m                              \u001b[0;34m\"activations are %s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                              % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "\u001b[0;31mValueError\u001b[0m: The activation '('relu', 'identity')' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh']."
     ]
    }
   ],
   "source": [
    "## Traning model\n",
    "mdl.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6f880ab-8569-4587-985a-a4e84fb80b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model evaluation\n",
    "ypred = mdl.predict(x,y, step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "284d3895-ef84-47a3-9274-f47d14927e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e1017f4-df5f-448d-bd01-807f2a106354",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subjects_trials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-211b21f776a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtrain_test_loops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-211b21f776a3>\u001b[0m in \u001b[0;36mtrain_test_loops\u001b[0;34m(hyperparams, fold_number, test_multil_trials)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#3) Load and normalize datasets for training and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msubjects_trials_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_subjects_trials_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_normalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#4) leave-one-out cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/PythonProject/DataAnalysis/P5/drop_landing_estimation/script/vicon_imu_data_process/process_rawdata.py\u001b[0m in \u001b[0;36mload_normalize_data\u001b[0;34m(hyperparams, scaler, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'columns_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'columns_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m     \u001b[0msubjects_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subjects_trials'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;31m#0) load raw datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subjects_trials'"
     ]
    }
   ],
   "source": [
    "7# Main rountine for developing ANN model for biomechanic variable estimations\n",
    "\n",
    "\n",
    "def train_test_loops(hyperparams=None, fold_number=1, test_multil_trials=False):\n",
    "    #1) set hyper parameters\n",
    "    if(hyperparams==None):\n",
    "        hyperparams = initParameters()\n",
    "    else:\n",
    "        hyperparams = hyperparams\n",
    "\n",
    "\n",
    "    #2) create a list of training and testing files\n",
    "    train_test_loop_folder = os.path.join(RESULTS_PATH, \"training_testing\",\n",
    "                                     str(localtimepkg.strftime(\"%Y-%m-%d\", localtimepkg.localtime())),\n",
    "                                     'train_test_loops', # many train and test loop based on cross validation\n",
    "                                     str(localtimepkg.strftime(\"%H_%M_%S\", localtimepkg.localtime()))\n",
    "                                    )\n",
    "    if(os.path.exists(train_test_loop_folder)==False):\n",
    "        os.makedirs(train_test_loop_folder)\n",
    "\n",
    "\n",
    "    # create file for storing train and test folders\n",
    "    train_test_loop_folders_log = os.path.join(train_test_loop_folder, \"train_test_loop_folders.log\")\n",
    "    if(os.path.exists(train_test_loop_folders_log)):\n",
    "        os.remove(train_test_loop_folders_log)\n",
    "\n",
    "    # create file for storing scores\n",
    "    cross_val_score_file = os.path.join(train_test_loop_folder, \"cross_validation_scores.csv\")\n",
    "    if(os.path.exists(cross_val_score_file)):\n",
    "        os.remove(cross_val_score_file)\n",
    "\n",
    "    # declare dictory to store training and testing folders\n",
    "    training_testing_results = {'training_folder':[],'testing_folder':[]}\n",
    "    cross_val_scores = []\n",
    "\n",
    "    #3) Load and normalize datasets for training and testing\n",
    "    subjects_trials_data, norm_subjects_trials_data, scaler = load_normalize_data(hyperparams)\n",
    "\n",
    "    #4) leave-one-out cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    loop_times = 0\n",
    "    subjects_trials = hyperparams['subjects_trials']\n",
    "    subject_ids_names = list(subjects_trials.keys())\n",
    "    cross_scores=[] # scores of cross validation\n",
    "    # check whether fold number is effective\n",
    "    if(fold_number>len(subject_ids_names)):\n",
    "        fold_number = len(subject_ids_names)\n",
    "\n",
    "    for train_subject_indices, test_subject_indices in loo.split(subject_ids_names):\n",
    "        #0) select model\n",
    "        model_type='tf_keras'\n",
    "        #i) declare model\n",
    "        model = model_v1(hyperparams)\n",
    "        #model = model_narx(hyperparams)\n",
    "\n",
    "        #ii) split dataset\n",
    "        train_set, valid_set, xy_test = split_dataset(norm_subjects_trials_data, train_subject_indices, test_subject_indices, hyperparams, model_type=model_type, multi_test_trials=test_multil_trials)\n",
    "\n",
    "        #iii) train model\n",
    "        trained_model, history_dict, training_folder = train_model(model, hyperparams, train_set, valid_set)\n",
    "        #trained_model, history_dict, training_folder = train_model_narx(model, hyperparams, train_set, valid_set)\n",
    "\n",
    "        #iv) test model\n",
    "        if(isinstance(xy_test, list)): # multi trials as test dataset\n",
    "            for trial_idx, a_trial_xy_test in enumerate(xy_test):\n",
    "                features, labels, predictions, testing_folder = es_as.test_model(training_folder, a_trial_xy_test, scaler)\n",
    "                training_testing_results['training_folder'].append(training_folder)\n",
    "                training_testing_results['testing_folder'].append(testing_folder)\n",
    "                cross_val_scores.append([loop_times, trial_idx] + list(es_as.calculate_scores(labels, predictions)))\n",
    "        else: # only a trial as test dataset\n",
    "            features, labels, predictions, testing_folder = es_as.test_model(training_folder, xy_test, scaler)\n",
    "            training_testing_results['training_folder'].append(training_folder)\n",
    "            training_testing_results['testing_folder'].append(testing_folder)\n",
    "            cross_val_scores.append([loop_times] + list(es_as.calculate_scores(labels, predictions)))\n",
    "\n",
    "        loop_times = loop_times + 1\n",
    "        if loop_times >= fold_number: # only repeat 4 times\n",
    "           break;# only run a leave-one-out a time\n",
    "\n",
    "    #5) cross validation scores on test dataset\n",
    "    cross_scores = np.array(cross_val_scores)\n",
    "    columns=['fold number', 'test_trial_idx', 'r2','mae','rmse','r_rmse']\n",
    "    pd_cross_scores = pd.DataFrame(cross_scores, columns=columns[-cross_scores.shape[1]:])\n",
    "    pd_cross_scores.to_csv(cross_val_score_file)\n",
    "    print('Cross validation mean r2 scores: {} on fold: {} cross validation on all test trials'.format(pd_cross_scores['r2'].mean(axis=0), fold_number))\n",
    "\n",
    "    #6) save train and test folder path\n",
    "    with open(train_test_loop_folders_log, 'w') as fd:\n",
    "        yaml.dump(training_testing_results, fd)\n",
    "\n",
    "    #training_testing_results['cross_val_scores'] = cross_scores\n",
    "    return training_testing_results, xy_test, scaler\n",
    "\n",
    "\n",
    "train_test_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609265e6-6866-4df2-983e-1333f0463a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
